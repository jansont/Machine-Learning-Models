{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Support Vector Machine\n",
    "\n",
    "As we saw in lectures, soft margin SVM uses **hinge loss** $L(y, z) = \\max(0, 1-yz)$. This is in contrast to the Perceptron's loss function $L(y,z) = \\max(0, -yz)$. In addition, while Perceptron uses SGD with a learning rate of $\\alpha=1$, we can choose other procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    \n",
    "    def __init__(self, learning_rate=.001, max_iters=1e4, epsilon=1e-8, record_history=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iters = max_iters\n",
    "        self.record_history = record_history\n",
    "        self.epsilon = epsilon\n",
    "        if record_history:\n",
    "            self.w_history = []\n",
    "            \n",
    "    def run(self, gradient_fn, x, y, w):\n",
    "        grad = np.inf\n",
    "        t = 1\n",
    "        while np.linalg.norm(grad) > self.epsilon and t < self.max_iters:\n",
    "            grad = gradient_fn(x, y, w)\n",
    "            w = w - self.learning_rate * grad\n",
    "            if self.record_history:\n",
    "                self.w_history.append(w)\n",
    "            t += 1\n",
    "        return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple implementation of Linear SVM, where the only difference with our previous implementation of logistic regression is the choice of loss function and the fact that the input labels are in $\\{-1,+1\\}$ rather than $\\{0,1\\}$ (note that in the implementation below, to keep things simple, we are applying the L2 regularization to the intercept as well.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_fn(x, y, w, lambdaa):\n",
    "    N, D = x.shape                                      # not really used!\n",
    "    z = np.dot(x, w)                                    # N\n",
    "    J = np.mean(np.maximum(0, 1- y*z)) + (lambdaa/2.) * np.linalg.norm(w)**2  #loss of the SVM\n",
    "    return J\n",
    "\n",
    "class LinearSVM:    \n",
    "    def __init__(self, add_bias=True, lambdaa = .01):\n",
    "        self.add_bias = add_bias\n",
    "        self.lambdaa = .01\n",
    "        pass\n",
    "            \n",
    "    def fit(self, x, y, optimizer):\n",
    "        if x.ndim == 1:\n",
    "            x = x[:, None]\n",
    "        if self.add_bias:\n",
    "            N = x.shape[0]\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        N,D = x.shape\n",
    "        y = 2*y - 1                                     # converting 0,1 to -1,+1\n",
    "        \n",
    "        def subgradient(x, y, w):\n",
    "            N,D = x.shape\n",
    "            yh = np.dot(x, w)\n",
    "            violations = np.nonzero(yh*y < 1)[0]                  # get those indexes for which yh*y<1\n",
    "            grad = -np.dot(x[violations,:].T, y[violations])/N    #compute x^Ty for those indexes and scale it down by N\n",
    "            grad += self.lambdaa * w                              #add the gradients from the weight regularization term\n",
    "            return grad\n",
    "\n",
    "        w0 = np.zeros(D)\n",
    "        self.w = optimizer.run(subgradient, x, y, w0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, x):\n",
    "        if self.add_bias:\n",
    "            x = np.column_stack([x,np.ones(N)])\n",
    "        yh = (np.sign(x@self.w) + 1)//2                           #converting -1,+1 to 0,1\n",
    "        return yh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try again to fit the Iris dataset of previous example this time using linear SVM; this is the setting where the data is not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_iris()\n",
    "x, y = dataset['data'][:,[1,2]], dataset['target']\n",
    "y =  y > 1\n",
    "optimizer = GradientDescent(learning_rate=.01, max_iters=300, record_history=True)\n",
    "model = LinearSVM(lambdaa=.00001)\n",
    "model.fit(x,y, optimizer)\n",
    "plt.plot(x[y==0,0], x[y==0,1], 'k.' )\n",
    "plt.plot(x[y==1,0], x[y==1,1], 'b.' )\n",
    "x_line = np.linspace(np.min(x[:,0]), np.max(x[:,0]), 100)\n",
    "for t,w in enumerate(optimizer.w_history):\n",
    "    coef = -w[0]/w[1]\n",
    "    plt.plot(x_line, coef*x_line - w[2]/w[1], 'r-', alpha=t/len(optimizer.w_history), label=f't={t}')#, alpha=(t+1)/(len(model.w_hist)+.1))\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.ylim(-1,10)\n",
    "plt.title('Perceptron when the data is not linearly separable')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('3.10.6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9754cddcf987eff3128efd44700e637bcc20314d850c0e47f5a6326ff0b0bf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
